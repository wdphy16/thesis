\chapter{Background}

\section{Phases in many-body systems}

The phases of matter are among the oldest and most mysterious physical phenomena. The solid, liquid, and gas phases are just elementary-school-level examples of the boarder classification of phases. Another widely known example is that magnets lose their magnetization when heated, which can be described as a transition from the ferromagnetic phase to the paramagnetic one. In the Landau theory of phase transitions~\cite{landau1937theory}, different phases are defined by their symmetries, and a phase transition occurs with a breaking of symmetry and is characterized by a corresponding order parameter. For example, the transition from the paramagnetic phase to the ferromagnetic phase breaks the spatial reflection symmetry, and magnetization serves as an order parameter.

This simple theory worked well until the discovery of the fractional quantum Hall effect~\cite{tsui1982two}, where the conductivity of a two-dimensional electron gas exhibits quantized values as the external magnetic field changes, and the difference between these values can be smaller than the minimal value predicted by the previous theory of energy level quantization. Perhaps more surprisingly, researchers attempted to distinguish the states with different conductivities into different phases, but only found that they all have the same symmetry and cannot be characterized by any local order parameter. Later, it was established that they can be characterized by global properties of the system, known as topological orders~\cite{wen1990ground}. Since then, topological phases of matter with exotic properties have found applications in various evolving fields of physics, including superconductivity~\cite{hansson2004superconductors}, nanoelectronics~\cite{gilbert2021topological}, and quantum computing~\cite{kitaev2003fault}.

A common kind of models for studying topological phases are the quantum spin liquid states~\cite{balents2010spin, misguich2011quantum, mila2015frustrated}. They are characterized by the absence of any magnetic order, and they possess long-range correlations to produce the topological orders. A particular example is the resonating valence bonds (RVB) state~\cite{anderson1973resonating}, which is defined by the superposition of all possible parings of electrons in the system. Spin liquids usually appear as the ground states of many-body systems with competing interactions, known as the frustration. These systems have been realized in experiments, such as the triangular lattice of YbMgGaO$_4$~\cite{li2015rare} and the kagome lattice of herbertsmithite~\cite{norman2016colloquium}. Meanwhile, the theoretical understanding of spin liquids is also under active development, such as the Kitaev model~\cite{kitaev2006anyons} on the hexagonal lattice with anisotropic interactions, which has been solved exactly. In addition to theoretical and experimental approaches, computational methods have been another line of research to predict the properties of these systems.

\section{Computational approach}

Since their invention, electronic computers have been executing one of the earliest algorithms in computational physics --- the Monte Carlo method~\cite{metropolis1949monte}, for those secret nuclear researches in World War II. This approach replaces the analytical integrals and summations in physical models by random sampling and obtains the results with the power of computers. Over the decades, our computational power has grown by orders of magnitude, and algorithms that once required hours on a supercomputer can be finished in minutes on a laptop today. However, the mere increase in computational power is nothing close to the computational complexity of many-body systems.

This complexity comes from the exponentially large state spaces. For example, a spin-$\frac{1}{2}$ particle has two possible states --- up and down, then a system of $N$ spins has $2^N$ possible states, which causes the so-called curse of dimensionality in statistics. The statistical properties of the system are given by summations over all the states, which would require exponentially long times if evaluated exactly, while in the viewpoint of computational complexity we only consider algorithms with polynomially long times practical. Therefore, rather than exact methods, we can only develop approximation methods to estimate these properties. The Monte Carlo is such a method known for avoiding the curse of dimensionality, where the variance of the estimation always decreases as the number of samples increases, regardless of the state space dimension.

In various schemes of Monte Carlo, we still need to deal with the complexity of the physical system apart from the dimensionality. The variational Monte Carlo (VMC)~\cite{scherer2017computational} is one of such schemes to study the ground states of quantum many-body systems, upon which this thesis mainly develops. It approximates the ground state wave function by a trial function, commonly referred to as the ``ansatz'', and optimizes the ansatz according to the sampled local energies of the physical system. Its difficulty is to construct an ansatz that can accurately approximate the target wave function, which can be a highly oscillating function in the exponentially high-dimensional state space, while evaluating and sampling from the ansatz in polynomial time. Traditional ansatzes usually fulfill these requirements with the mathematical properties of determinants~\cite{slater1929theory, bouchaud1988pair}, and it had been a long and difficult search to construct more powerful ansatzes from physical arguments.

\section{Neural networks}
\label{sec:intro-ml}

The development of neural networks in the past decade has shed new light on this area. Neural networks, in the context of modern machine learning, are a diverse class of mathematical functions that are historically inspired by the functionality of biological neural networks~\cite{hopfield1982neural, mackay2003information}. A striking difference from previous machine learning approaches is that neural networks typically have numerous parameters, where modern large models have reached hundreds of billions of parameters~\cite{brown2020language}. In theory, they are universal approximators of any well-behaved functions if optimized w.r.t.\ the parameters~\cite{hornik1989multilayer}. Although the ideal optimization of so many parameters is intractable in practice, they can still be far more expressive than traditional function approximators~\cite{sontag1998vc}. On the other hand, the large number of parameters makes them more computationally expensive than traditional approaches, and they are usually executed on graphics processing units (GPUs) and other specialized hardware~\cite{chen2020survey}. The optimization of these parameters is known as ``training'' or ``learning'', highlighting the paradigmatic difference from traditional optimization problems with few variables.

Since their breakthrough on image classification problems~\cite{krizhevsky2012imagenet}, the neural networks with deep multilayer architectures have opened a new era of machine learning studies, known as ``deep learning''~\cite{goodfellow2016deep}. They have dominated state-of-the-art results in all machine learning domains and led to the growth of the data-driven paradigm in vast scientific fields~\cite{montans2019data}, where prominent examples include the modeling of languages~\cite{brown2020language}, images~\cite{rombach2022high}, and strategic games~\cite{silver2016mastering}. These successful results are conjectured to stem from the high efficiency of neural networks to represent complicated probability distributions in exponentially high-dimensional spaces --- probabilities of valid speeches in the space of many words, probabilities of recognizable images in the space of many pixels, and probabilities of winning moves in the space of many game board states. Therefore, they have also been introduced as promising tools in many-body physics, where such distributions have become the main difficulty.

\section{Outline}

This thesis is written in a mostly self-contained way, assuming that the reader has basic backgrounds in statistics and quantum mechanics. It is organized as follows:
\begin{itemize}
\item \Cref{ch:systems} provides the definitions of the classical and quantum many-body systems studied in this thesis, including the classical Ising model and its variants, the quantum Heisenberg model, and the transverse-field Ising model (TFIM).
It discusses the brief histories and the motivations to study them, and the issues of geometrical frustration and random interactions that create disordered states of the systems. These systems exhibit rich and exotic physical phenomena, which pose challenge to the computational studies we will present.
\item \Cref{ch:mcmc} starts the discussion of computational studies on classical many-body systems. It reviews the Markov chain Monte Carlo (MCMC) method, which is the most traditional and widely used method to stochastically estimate the observables under the complicated many-body distributions of physical systems.
It discusses the issues of autocorrelation time, critical slowing down, burn-in stage before equilibrium, and mode collapse, which will reoccur in all other methods based on MCMC, as well as in quantum systems.
Before introducing the Markov chain sampling, it also defines the concept of exact sampling with simple examples, which will be the focus of this thesis.
\item \Cref{ch:cl-var} reviews the variational method to approximate the Boltzmann distributions of classical many-body systems.
It gives detailed explanations on the optimization methods and the gradient estimator for the variational free energy, as well as a comparison to MCMC.
Then it lists some commonly used variational ansatzes, including the naive mean-field ansatz, the Bethe ansatz, and neural network ansatzes, with a self-contained formulation of neural networks.
This variational method has close resemblance to the variational Monte Carlo method for quantum systems in \cref{ch:vmc}.
\item \Cref{ch:arnn} introduces the autoregressive neural networks (ARNNs). Using the power of neural networks, they enable practical applications of exact sampling for complicated many-body distributions.
It presents the basic architectures of ARNN from Ref.~\cite{wu2019solving}, the sparse two-body ARNN (TwoBo) from Ref.~\cite{biazzo2024sparse} to substantially improve the efficiency using the sparsity structure of the physical system, and the neural cluster updates with symmetries (NCUS) from Ref.~\cite{wu2021unbiased} to remove the bias from the variational approximation, with numerical results demonstrating their accuracy and efficiency.
\item \Cref{ch:qmc} moves the discussion to computational studies on quantum many-body systems. It reviews the unbiased methods to evaluate the ground state, including the theoretical formulation of imaginary time evolution (ITE), the exact diagonalization (ED) method derived from the former, and the quantum Monte Carlo (QMC) methods, in particular the path integral Monte Carlo (PIMC).
It discusses the impact of the energy gap size on the convergence speeds of these methods, and the notorious sign problem, which will reoccur in \cref{ch:vmc}.
The results of ED and QMC are used as baselines for other numerical studies, including the benchmarks in \cref{ch:varbench}.
\item \Cref{ch:vmc} reviews the variational Monte Carlo (VMC) method, which is more widely used in quantum problems than the variational method in classical problems, because VMC has higher efficiency than the unbiased QMC schemes.
It skips the formulation similar to the classical variational method, and mentions the relation between the variance and the accuracy of the variational energy, which will be the fundament of the benchmarks in \cref{ch:varbench}.
It emphasizes the particular difficulty of complex gradients for wave functions, and presents the stochastic reconfiguration (SR) method to help the optimization in complicated energy landscapes of quantum systems.
Then it lists some commonly used variational ansatzes, including the Jastrow ansatz, the Gutzwiller projected states, and the neural quantum states (NQSs), with a discussion on parameterizing and optimizing the phase of the wave function.
\item \Cref{ch:tn} reviews the tensor network (TN) method, which is an alternative to VMC for the variational approximation of quantum states. TNs have well-understood physical properties, such as entanglement entropy and spatial correlations, as well as the advantage of exact contraction if the computational cost allows.
In particular, it presents the matrix product state (MPS) and the matrix product operator (MPO) architectures in 1D, as well as the density matrix renormalization group (DMRG) algorithm to optimize MPS, with brief introductions to architectures in 2D and higher.
\item \Cref{ch:tensor-rnn} introduces the tensor-RNN from Ref.~\cite{wu2023tensor}, a variational ansatz  combining the strengths of TN in \cref{ch:tn} and NQS in \cref{ch:vmc}. This ansatz supports exact sampling and efficient evaluation, captures the desired analytical properties of entanglement entropy and spatial correlations in 2D, and produces systematically improved accuracy as the computational budget increases. These advantages are supported by both theoretical and numerical evidence.
\item \Cref{ch:varbench} introduces the VarBench project from Ref.~\cite{wu2024variational}. It first proposes the V-score, a universal metric based on the variance of variational energy, which measures the accuracy of any variational approximation, as well as the hardness of simulating any Hamiltonian. This score is used in an extensive collection of benchmarks for many kinds of variational methods and quantum many-body systems, which identifies certain hard Hamiltonians that are not yet well solved, and can be the targets to demonstrate the quantum advantage.
\item \Cref{ch:conclusion} concludes the thesis, summarizes the main results that improve the accuracy and the efficiency of computational studies in both classical and quantum many-body systems, and poses directions and open questions for further research.
\end{itemize}
