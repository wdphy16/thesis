\chapter{Variational Monte Carlo (VMC)}
\label{ch:vmc}

The PIMC method in the previous chapter allows us to obtain an unbiased stochastic approximation of the ground state properties for quantum many-body problems, in the same manner as the MCMC method in \cref{ch:mcmc}. Apart from that approach, another QMC scheme has been proposed to obtain a variational approximation of the target system, which bears the name variational Monte Carlo (VMC)~\cite{scherer2017computational, sorella2005wave} and resembles the variational method in \cref{ch:cl-var}. Unlike classical systems where MCMC suffices to obtain accurate results in many cases, in quantum systems the sheer large amount of computation required by unbiased methods has motivated a wide usage of variational methods, as we will see in the following.

\section{Variational energy}

The ground state of a quantum system in \cref{eq:gs} is already defined by the variational principle, and we only need to rewrite it in a form suitable for Monte Carlo summation. Let $\psi(\vs)$ be the variational ansatz, which is not necessarily normalized, then its energy can be written as
\begin{align}
E_\psi &= \frac{\ev{\hat{H}}{\psi}}{\ip{\psi}} \\
&= \frac
{\sum_{\vs, \vs'} \ip{\psi}{\vs} \mel{\vs}{\hat{H}}{\vs'} \ip{\vs'}{\psi}}
{\ip{\psi}} \\
&= \frac
{\sum_\vs \ip{\psi}{\vs} \ip{\vs}{\psi} \sum_{\vs'} \mel{\vs}{\hat{H}}{\vs'} \frac{\ip{\vs'}{\psi}}{\ip{\vs}{\psi}}}
{\ip{\psi}} \\
&= \sum_\vs q(\vs) E_\text{loc}(\vs), \label{eq:vmc}
\end{align}
where the probability
\begin{equation}
q(\vs) = \frac{|\psi(\vs)|^2}{\ip{\psi}},
\end{equation}
and the local energy
\begin{equation}
E_\text{loc}(\vs) = \sum_{\vs'} H(\vs, \vs') \frac{\psi(\vs')}{\psi(\vs)}.
\end{equation}

The variational energy in \cref{eq:vmc} has a similar form to the variational free energy in \cref{eq:fq}. In the same manner discussed there, we can generate samples of the configuration $\vs$ from $q(\vs)$, estimate $E_\psi$ using the Monte Carlo summation, then minimize it w.r.t.\ the parameters in $\psi(\vs)$ using gradient-based optimizers, and a lower $E_\psi$ indicates a better ansatz to approximate the ground state. It is worth mentioning that the state $\ket{\psi}$ obtained from variational optimization can be further improved by iterative eigenvector solvers in \cref{sec:ed}~\cite{hu2013direct, chen2022systematic}, or used as the initial state for other QMC schemes in \cref{sec:qmc}.

When the Hamiltonian is non-stoquastic, unlike the weight $\varPi(\svs)$ for PIMC in \cref{eq:pimc}, in VMC we always have $q(\vs) \ge 0$. However, the sign problem can still occur in the sense that $E_\text{loc}(\vs)$ can be either positive or negative, which further causes large noise in the estimated gradient and impedes the optimization.

\section{Energy variance}
\label{sec:vmc-var}

Besides the energy itself, its variance $\Var E_\psi = \ev{\hat{H}^2} - \ev{\hat{H}}^2$ under the state $\ket{\psi}$ can also be estimated by Monte Carlo sampling, where the first term can be written as
\begin{align}
\frac{\ev{\hat{H}^2}{\psi}}{\ip{\psi}}
&= \frac
{\sum_{\vs, \vs', \vs''} \ip{\psi}{\vs'} \mel{\vs'}{\hat{H}}{\vs} \mel{\vs}{\hat{H}}{\vs''} \ip{\vs''}{\psi}}
{\ip{\psi}} \label{eq:vmc-var-1} \\
&= \frac
{\sum_\vs \ip{\psi}{\vs} \ip{\vs}{\psi}
\left( \sum_{\vs'} \frac{\ip{\psi}{\vs'}}{\ip{\psi}{\vs}} \mel{\vs'}{\hat{H}}{\vs} \right)
\left( \sum_{\vs''} \mel{\vs}{\hat{H}}{\vs''} \frac{\ip{\vs''}{\psi}}{\ip{\vs}{\psi}} \right)}
{\ip{\psi}} \label{eq:vmc-var-2} \\
\shortintertext{(Assuming $\ip{\vs}{\psi} \neq 0$ for all $\vs$)}
&= \sum_\vs q(\vs) |E_\text{loc}(\vs)|^2. \label{eq:vmc-var}
\end{align}
Therefore, the variance of the energy happens to be the same as the variance of the local energy in \cref{eq:vmc}. This variance has the noteworthy property that it reaches zero when the variational state $\ket{\psi}$ reaches the ground state $\ket{\psi_0}$, which is useful to indicate the progress of the variational optimization. For a simple example, in a two-state system $\ket{\psi} = \sqrt{\lambda} \ket{\psi_0} + \sqrt{1 - \lambda} \ket{\psi_1}$, where $\lambda \in [0, 1]$ is a tunable parameter, we have
\begin{equation}
\Var E_\psi = \lambda (1 - \lambda) \Delta E^2, \label{eq:var-two-states}
\end{equation}
where $\Delta E = E_1 - E_0$ is the energy gap, and we can see $\Var E_\psi = 0$ when $\lambda = 1$. However, the variance is also zero if $\ket{\psi}$ is trapped in any excited state, as we can see $\Var E_\psi = 0$ when $\lambda = 0$ in \cref{eq:var-two-states}. This will be discussed in more depth in \cref{ch:varbench}.

Caution should be taken that we have assumed $\ip{\vs}{\psi} \neq 0$ for all $\vs$ when inserting $\ip{\vs}{\psi}$ into the denominators in \cref{eq:vmc-var-2}. If this assumption is not fulfilled, then the summation in \cref{eq:vmc-var} will only contain the terms with $\ip{\vs}{\psi} \neq 0$, and the variance of the local energy will be a biased estimator of the true variance, as pointed out in Ref.~\cite{sinibaldi2023unbiasing}. In practice, when some $\ip{\vs}{\psi}$ are exponentially suppressed and numerically close to zero, they can cause high variance of the variance estimator, which means that the estimated variance and therefore the convergence of the optimization is unreliable. For example, if the ansatz is pathologically concentrated such that we can only sample a single configuration $\vs$ from it, then the local energy $E_\text{loc}(\vs)$ always has the same value, and the estimated $\Var E_\psi$ is zero. This problem is similar to the mode collapse aforementioned in \cref{sec:mode-collapse}, and in the quantum case it can occur even though we are using the variational approximation rather than the unbiased approximation. Note that this problem does not occur when estimating the energy, because we have $\lim_{\psi(\vs) \to 0} q(\vs) E_\text{loc}(\vs) = 0$, while it occurs in the variance, because $\psi(\vs)$ cancels out in the numerator and the denominator in $q(\vs) |E_\text{loc}(\vs)|^2$.

\section{Complex gradient}
\label{sec:cmpl-grad}

Similar to \cref{eq:fq-grad}, the gradient of $E_\psi$ also needs to be derived in a form suitable for Monte Carlo summation. In the general case where the Hamiltonian $\hat{H}$, the wave function $\psi(\vs)$, and the parameters $\theta$ can be complex valued, we have
\begin{equation}
\frac{\partial E_\psi}{\partial \theta} = \sum_\vs q(\vs) \left( \left( E_\text{loc}(\vs) - E_\psi \right)^* \frac{\partial \ln \psi(\vs)}{\partial \theta} + \left( E_\text{loc}(\vs) - E_\psi \right) \frac{\partial \ln \psi^*(\vs)}{\partial \theta} \right), \label{eq:vmc-grad-cmpl}
\end{equation}
whose derivation is presented in \cref{append:vmc-grad}. In the simple case where $\hat{H}$, $\psi(\vs)$, and $\theta$ are all real-valued, it simplifies to
\begin{equation}
\frac{\partial E_\psi}{\partial \theta} = 2 \sum_\vs q(\vs) \left( E_\text{loc}(\vs) - E_\psi \right) \vD(\vs),
\label{eq:vmc-grad-real}
\end{equation}
where $\vD(\vs) = \frac{\partial \ln \psi(\vs)}{\partial \theta}$ denotes the gradient vector of $\ln \psi(\vs)$. To reduce the variance when estimating the gradient using Monte Carlo sampling, in the same manner as \cref{eq:fq-grad-baseline}, we shift $\vD(\vs)$ to have zero mean, without changing the expectation of the gradient:
\begin{align}
\frac{\partial E_\psi}{\partial \theta} &= 2 \sum_\vs q(\vs) \left( E_\text{loc}(\vs) - E_\psi \right) \left( \vD(\vs) - \bar{\vD} \right), \label{eq:vmc-grad-baseline} \\
\bar{\vD} &= \sum_\vs q(\vs) \vD(\vs).
\end{align}

A peculiar nature of quantum systems is that $\hat{H}$, $\psi(\vs)$, and $\theta$ can actually be complex valued, which has caused substantial confusion because there is no universally accepted and applicable definition for the gradient of a complex-valued function w.r.t.\ complex parameters. For example, the usual definition in complex analysis requires that $\lim_{\Delta z \to 0} \frac{f(z + \Delta z) - f(z)}{\Delta z}$ exists for all directions of $\Delta z$ on the complex plane~\cite{rudin1986real}, which is usually unfulfilled for complicated functions such as neural networks~\cite{bassey2021survey}. Even if this condition is fulfilled, it usually implies that $f(z)$ is holomorphic and has singular points, which impedes the numerical stability of the variational optimization.

Fortunately, the variational energy $E_\psi$ is guaranteed to be real when the summation in \cref{eq:vmc} is performed exactly, which allows us to define a ``split'' gradient for the purpose of the gradient descent (GD) optimizer in \cref{eq:gd}. Assuming $\theta = \thetar + \rmi \thetai$,  we compute the gradient of the real function $E_\psi$ w.r.t.\ the real parameters $\thetar$ and $\thetai$ respectively, then combine them by
\begin{equation}
\frac{\partial E_\psi}{\partial \theta} = \frac{\partial E_\psi}{\partial \thetar} + \rmi \frac{\partial E_\psi}{\partial \thetai},
\end{equation}
which yields the correct direction of optimization when substituted into \cref{eq:gd}. Using this definition, the gradient in \cref{eq:vmc-grad-cmpl} becomes
\begin{align}
\frac{\partial E_\psi}{\partial \theta}
&= \sum_\vs q(\vs) \bigg(
\left( E_\text{loc}(\vs) - E_\psi \right)^* \ptri \ln \psi(\vs) \nonumber \\
&\phantom{{}={}} \qquad\quad + \left( E_\text{loc}(\vs) - E_\psi \right) \ptri \ln \psi^*(\vs)
\bigg).
\end{align}
Because $\frac{\partial f^*(x)}{\partial x} = \left( \frac{\partial f(x)}{\partial x} \right)^*$ when $x$ is real, we have
\begin{align}
\frac{\partial E_\psi}{\partial \theta}
&= \phantom{+ \rmi}\!\sum_\vs q(\vs) \left(
\left( E_\text{loc}(\vs) - E_\psi \right)^* \frac{\partial \ln \psi(\vs)}{\partial \thetar}
+ \left( E_\text{loc}(\vs) - E_\psi \right) \left( \frac{\partial \ln \psi(\vs)}{\partial \thetar} \right)^*
\right) \nonumber \\
&\phantom{=} + \rmi \sum_\vs q(\vs) \left(
\left( E_\text{loc}(\vs) - E_\psi \right)^* \frac{\partial \ln \psi(\vs)}{\partial \thetai}
+ \left( E_\text{loc}(\vs) - E_\psi \right) \left( \frac{\partial \ln \psi(\vs)}{\partial \thetai} \right)^*
\right) \\
% &= \phantom{{}+ \rmi \cdot{}} 2 \Re \sum_\vs q(\vs)
% \left( E_\text{loc}(\vs) - E_\psi \right)
% \left( \frac{\partial \ln \psi(\vs)}{\partial \thetar} \right)^* \nonumber \\
% &\phantom{{}={}} + \rmi \cdot 2 \Re \sum_\vs q(\vs)
% \left( E_\text{loc}(\vs) - E_\psi \right)
% \left( \frac{\partial \ln \psi(\vs)}{\partial \thetai} \right)^*, \\
&= 2 \sum_\vs q(\vs) \left(
\Re\left( \left( E_\text{loc}(\vs) - E_\psi \right) \left( \frac{\partial \ln \psi(\vs)}{\partial \thetar} \right)^* \right)
+ \rmi \Re\left( \left( E_\text{loc}(\vs) - E_\psi \right) \left( \frac{\partial \ln \psi(\vs)}{\partial \thetai} \right)^* \right)
\right). \label{eq:vmc-grad-split}
\end{align}
In addition, we can reduce its variance in the same manner as \cref{eq:vmc-grad-baseline}.

The same gradient, except differing by an overall coefficient of $2$ because of the convention, can be derived from the viewpoint of the Wirtinger calculus~\cite{wirtinger1927formalen}, also known as the $\bbC \bbR$-calculus~\cite{kreutz2009complex}. In this theory, any complex-valued function can be written as $f(z, z^*)$, which is holomorphic in $z$ and $z^*$ respectively, and we can take the gradient $\frac{\partial f}{\partial z}$ and the co-gradient $\frac{\partial f}{\partial z^*}$ while treating the other variable as constant. It has been proven that
\begin{equation}
\frac{\partial f}{\partial z} = \frac{1}{2} \left( \frac{\partial f}{\partial x} - \rmi \frac{\partial f}{\partial y} \right), \quad
\frac{\partial f}{\partial z^*} = \frac{1}{2} \left( \frac{\partial f}{\partial x} + \rmi \frac{\partial f}{\partial y} \right),
\end{equation}
where $z = x + \rmi y$. For the purpose of GD, we substitute the co-gradient $\frac{\partial E_\psi}{\partial \theta^*}$ into \cref{eq:gd}, which is the same as \cref{eq:vmc-grad-split} except without the coefficient of $2$. Automatic differentiation software can use the Wirtinger calculus with the chain rule of derivatives to compute $\frac{\partial \ln \psi}{\partial \theta^*}$~\cite{kramer2024tutorial}, which usually takes less computation time than computing $\frac{\partial \ln \psi}{\partial \thetar}$ and $\frac{\partial \ln \psi}{\partial \thetai}$ separately, but care should be taken that different software can have different conventions for the sign of the imaginary part and the overall coefficient.

It is worth discussing the $\Re$ notation in \cref{eq:vmc-grad-split}. The gradients $\frac{\partial E_\psi}{\partial \thetar}$ and $\frac{\partial E_\psi}{\partial \thetai}$ are both real when evaluated exactly, because they are gradients of a real-valued function w.r.t.\ real parameters. Therefore, the sums $\sum_\vs q(\vs) \left( E_\text{loc}(\vs) - E_\psi \right) \left( \frac{\partial \ln \psi(\vs)}{\partial \thetar} \right)^*$ and $\sum_\vs q(\vs) \left( E_\text{loc}(\vs) - E_\psi \right) \left( \frac{\partial \ln \psi(\vs)}{\partial \thetai} \right)^*$ are also real when evaluated exactly. However, when using Monte Carlo sampling, the estimators $\bbE_\text{MC}\left[ \left( E_\text{loc} - E_\psi \right) \left( \frac{\partial \ln \psi}{\partial \thetar} \right)^* \right]$ and $\bbE_\text{MC}\left[ \left( E_\text{loc} - E_\psi \right) \left( \frac{\partial \ln \psi}{\partial \thetai} \right)^* \right]$ can have imaginary parts because of the discrepancy of samples, which are discarded in \cref{eq:vmc-grad-split} before outputting the gradient and updating the parameters in \cref{eq:gd}.

In some implementations of VMC, as well as stochastic gradient descent (SGD) in other complex-valued optimization problems, the $\Re$ notation is ignored either deliberately or unintentionally, and the imaginary parts of the estimated $\frac{\partial E_\psi}{\partial \thetar}$ and $\frac{\partial E_\psi}{\partial \thetai}$ are mixed in the updates to $\thetai$ and $\thetar$, respectively. This additional noise in the gradient may affect the convergence of SGD, as discussed in \cref{sec:gd}. To the author's knowledge, there is no strong evidence that either implementation consistently outperforms the other. However, an occasionally occurring mistake is to take the real parts before multiplying, i.e., $\Re\left( E_\text{loc} - E_\psi \right) \Re\left( \frac{\partial \ln \psi}{\partial \thetar} \right)$, which changes the result even if the summation over $\vs$ is performed exactly. We refer to Ref.~\cite{bassey2021survey} as a recent survey on complex-valued neural networks, including their optimization, in various fields of machine learning.

\section{Stochastic reconfiguration (SR)}
\label{sec:sr}

Beyond the simple GD optimizer, a commonly used optimization method in VMC is stochastic reconfiguration (SR)\footnote{The name ``stochastic reconfiguration'' is also used in diffusion Monte Carlo (DMC), where it refers to the technique to reconfigure the number of walkers~\cite{assaraf2000diffusion}. Both names originate from the earlier work on Green's function Monte Carlo~\cite{sorella1998green}.}. It is naturally derived as an approximation of the imaginary time evolution (ITE) in \cref{sec:ite}, and is similar to the natural gradient descent (NGD) in \cref{sec:ngd}.

We consider the ITE of the state $\ket{\psi}$ with a small step $\Delta \tau$:
\begin{equation}
\ket{\psi_{\Delta \tau}} = \rme^{-\Delta \tau \hat{H}} \ket{\psi} = \ket{\psi} - \Delta \tau \hat{H} \ket{\psi} + O(\Delta \tau^2),
\end{equation}
and project it onto a Hilbert subspace generated by the states $\left\{ \ket{\psi}, \ket{\partial_1 \psi}, \ket{\partial_2 \psi}, \ldots, \ket{\partial_{n_\text{p}} \psi} \right\}$, where we denote $\ket{\partial_1 \psi} = \frac{\partial}{\partial_{\theta_i}} \ket{\psi}$, and $n_\text{p}$ is the number of parameters. This space contains all the states that the ansatz can reach by varying the parameters, up to the local linear approximation. To make the projection, we try to write $\ket{\psi_{\Delta \tau}}$ as a linear combination of these basis states:
\begin{equation}
\ket{\psi_{\Delta \tau}} = \alpha \ket{\psi} + \sum_i \Delta \theta_i \ket{\partial_i \psi},
\label{eq:psi-dtau-proj}
\end{equation}
where $\{\Delta \theta_i\}$ will be used as the updates to the parameters, and $\alpha$ represents the change in the magnitude of $\ket{\psi}$. The dimension of the subspace $n_\text{p} + 1$ is usually much less than that of the whole Hilbert space $2^N$, so \cref{eq:psi-dtau-proj} does not hold exactly, but we require it to hold when multiplying both sides with each basis state:
\begin{align}
\ip{\psi}{\psi_{\Delta \tau}} &= \alpha \ip{\psi} + \sum_i \Delta \theta_i \ip{\psi}{\partial_i \psi}, \label{eq:sr-eq-1} \\
\ip{\partial_j \psi}{\psi_{\Delta \tau}} &= \alpha \ip{\partial_j \psi}{\psi} + \sum_i \Delta \theta_i \ip{\partial_j \psi}{\partial_i \psi}. \label{eq:sr-eq-2}
\end{align}
Cancelling $\alpha$, and ignoring $O(\Delta \tau^2)$ terms, the equations above simplify to
\begin{equation}
\mS \Delta \theta = -\frac{\Delta \tau}{2} \vf,
\label{eq:sr-eq}
\end{equation}
where $\vf$ is the gradient in \cref{eq:vmc-grad-real}, commonly known as the ``force'' in the context of VMC, and $\mS$ is an $n_\text{p} \times n_\text{p}$ matrix define by
\begin{equation}
S_{i j} = \sum_\vs q(\vs) \left( D_i(\vs) - \bar{D}_i \right)^* \left( D_j(\vs) - \bar{D}_j \right).
\label{eq:qgt}
\end{equation}
The details on the derivation of \cref{eq:sr-eq} are presented in \cref{append:sr}. It has the same form in the real and the complex cases, as long as the same definition of the complex gradient is used in $\mS$ and $\vf$. In the same form as the preconditioned GD in \cref{eq:newton}, the parameters are updated by
\begin{equation}
\theta_{t + 1} \gets \theta_t - \gamma \left. \mS^{-1} \vf \right|_{\theta = \theta_t},
\label{eq:sr}
\end{equation}
where $\gamma = \frac{\Delta \tau}{2}$ is the learning rate. Therefore, the SR algorithm is also known as the quantum natural gradient descent~\cite{stokes2020quantum}. Alternatively, it can be derived from the viewpoint of the Fubini--Study metric~\cite{study1905kurzeste}, which replaces the classical case of the KL divergence in \cref{eq:kl}, and $\mS$ is called the quantum geometric tensor (QGT)~\cite{berry1989quantum}, which generalizes the Fisher information matrix in \cref{eq:fim} into the quantum case.

In practice, we usually need to estimate $\mS$ along with $\vf$ using Monte Carlo sampling:
\begin{align}
\theta_{t + 1} &\gets \theta_t - \gamma \left. \tilde{\mS}^{-1} \tilde{\vf} \right|_{\theta = \theta_t}, \\
\tilde{\mS} &= \frac{1}{M} \sum_{i = 1}^M
\left( \vD\big( \vs^{(i)} \big) - \tilde{\vD} \right)^*
\left( \vD\big( \vs^{(i)} \big) - \tilde{\vD} \right)^\transpose, \label{eq:qgt-mc} \\
\tilde{\vf} &= \frac{2}{M} \sum_{i = 1}^M
\left( E_\text{loc}\big( \vs^{(i)} \big) - \tilde{E} \right)
\left( \vD\big( \vs^{(i)} \big) - \tilde{\vD} \right)^*, \\
\tilde{E} &= \frac{1}{M} \sum_{i = 1}^M E_\text{loc}\big( \vs^{(i)} \big), \\
\tilde{\vD} &= \frac{1}{M} \sum_{i = 1}^M \vD\big( \vs^{(i)} \big), \quad
\vs^{(i)} \sim q,
\end{align}
where $M$ is the number of samples, and usually $M < n_\text{p}$. In this case, a problem occurs that each term $\vD(\vs)^* \vD(\vs)^\transpose$ is a rank-$1$ matrix, so $\tilde{\mS}$ has a rank of at most $M$, and it cannot be inverted in \cref{eq:sr}. A common remedy is to add a diagonal term to $\tilde{\mS}$ before inverting it:
\begin{equation}
\theta_{t + 1} \gets \theta_t - \gamma \left. (\tilde{\mS} + \lambda \mI)^{-1} \tilde{\vf} \right|_{\theta = \theta_t}.
\label{eq:sr-ds}
\end{equation}
It is technically known as the Tikhonov regularization~\cite{tikhonov1943stability} and colloquially referred to as the ``diagonal shift'', and it also has resemblance to the Levenberg--Marquardt algorithm discussed in \cref{sec:ngd} as an interpolation between the first- and the second-order updates. In the extreme case that $\lambda \to \infty$ while $\frac{\gamma}{\lambda}$ is fixed, \cref{eq:sr-ds} degenerates to the simple GD in \cref{eq:gd}. In addition to the learning rate $\gamma$, the diagonal shift $\lambda$ is another crucial hyperparameter in VMC that usually requires manual tuning. We may use a large $\lambda$ at the beginning of the optimization to ensure stability, and decrease it as the optimization approaches the minimum.

Recently, a different approach to invert $\tilde{\mS}$ has been proposed in Refs.~\cite{chen2023efficient, rende2024simple}. The estimated update to the parameters can be expressed as
\begin{equation}
\theta_{t + 1} \gets \theta_t - \gamma \left. (\mX \mX^\dagger + \lambda \mI)^{-1} \mX \tilde{\vf} \right|_{\theta = \theta_t},
\label{eq:minsr}
\end{equation}
which has the same form as the common definition of the pseudoinverse of a singular matrix, where $\mX$ is a $M \times n_\text{P}$ matrix defined by
\begin{equation}
X_{i j} = D_j\left( \vs^{(i)} \right) - \tilde{D}_j.
\end{equation}
This method is named MinSR in Ref.~\cite{chen2023efficient}, and it is gradually becoming the default implementation of SR when $M < n_\text{p}$. Note that even if $\mX \mX^\dagger$ is full rank, the diagonal shift $\lambda$ is usually still needed to improve stability, because the limited number of samples can cause substantial noise in $\tilde{\mS}$. It is worth mentioning that previously the large size of $\tilde{\mS}$ requires us to use iterative solvers for the linear system in \cref{eq:sr-eq}, which avoid storing all the $n_\text{p} \times n_\text{p}$ entries in $\tilde{\mS}$. As the size of the matrix to reduce is substantially smaller in MinSR, we can use exact solvers instead, which enable more efficient parallelization on GPUs, and have fewer hyperparameters prone to manual tuning.

However, $\mX \mX^\dagger$ can still be singular if the expressiveness of the ansatz is limited, which is indicated by the degeneracy in the basis vectors of the locally linearized variational space in \cref{eq:psi-dtau-proj}. Worse still, the magnitude of $\psi(\vs)$ can be exponentially suppressed for many $\vs$, which causes numerically zero eigenvalues in $\mS$, and the number of numerically non-zero eigenvalues can be less than $M$ when the wave function is highly concentrated. These zero or near-zero eigenvalues correspond to zero or near-zero entries in $\mX \tilde{\vf}$ in principle, but the estimated linear system can contain noise that leads to a pathological solution of $\tilde{\mS}^{-1} \tilde{\vf}$. Therefore, extra care should be taken when inverting $\tilde{\mS}$. In addition to diagonal shift, it has been proposed to truncate the near-zero eigenvalues before the pseudoinverse, either with a fixed threshold or using the signal-to-noise ratio (SNR) of $\tilde{\mS}$~\cite{schmitt2020quantum, chen2023efficient}. Alternatively, Ref.~\cite{goldshlager2024kaczmarz} has proposed a running estimation of $\tilde{\mS}^{-1}$ over multiple optimization steps, which increases the effective sample size compared to inverting it in each step.

It has been shown that SR is necessary to obtain correct solutions to non-stoquastic systems in many cases~\cite{carleo2017solving, choo2019two, bukov2021learning}, and the simple SGD or its variants such as Adam fail to converge to the ground state. This success is attributed to the incorporation of the local geometry in SR, which smoothens the highly non-convex energy landscape. In the ideal case where the estimations of $\vf$ and $\mS$ are exact, the ansatz has sufficiently high expressiveness, and the learning rate $\gamma$ is sufficiently small, then SR is equivalent to the exact ITE and always converges to the ground state after sufficiently many steps, as long as the initial state is valid.

It is also commonly believed that SR should not be used together with optimizers employing adaptive learning rates, such as Adam, because they make SR deviate from the ideal convergence path of ITE. On the other hand, multistep solvers for ordinary differential equations (ODEs), such as the Heun and the Runge--Kutta schemes, have been proposed to replace the single-step update in \cref{eq:sr}~\cite{bukov2021learning}. They improve the order of approximation to ITE and are more commonly used in literature on dynamics~\cite{schmitt2020quantum}, although the limited accuracy in estimating $\mS$ still impedes their performance.

After the above introduction to the general framework of VMC, in the following we discuss some common choices of the variational ansatz for quantum spin-$\frac{1}{2}$ systems that are of interest in this thesis.

\section{Variational ansatzes for quantum spin systems}

\subsection{Jastrow ansatz}
\label{sec:jastrow}

The Jastrow ansatz~\cite{jastrow1955many} was originally proposed to model the interactions between particles depending on their relative distances. In the context of spin systems, it is a quadratic form of the input configuration, without the normalization~\cite{huse1988simple}:
\begin{equation}
\ln \psi_\text{J}(\vs) = \frac{1}{2} \vs^\transpose \mW^\text{J} \vs,
\end{equation}
where $\mW^\text{J}$ is an $N \times N$ matrix of tunable parameters. Despite its simplicity, it is capable of modeling two-spin correlations and providing some initial insights into the system. The symmetries of the system can be imposed in the structure of $\mW^\text{J}$.

\subsection{Gutzwiller projected states}
\label{sec:gutz}

Quantum many-body systems can exhibit a plethora of collective and emergent behaviors beyond two-spin correlations, such as spinons and fractional excitations in spin liquids. Although we mainly focus on spin systems in this thesis, an approach to study these systems is to transform the Hamiltonian of spins to an equivalent Hamiltonian of fermions, then it is possible to construct highly entangled states using the anti-commutation relations of fermions, which produce the necessary many-body correlations to model the aforementioned intricate behaviors, while can be evaluated in polynomial time. These introduced fermions are known as partons, namely the Abrikosov fermions~\cite{abrikosov1965electron} in the following case. The transformation is given by
\begin{equation}
\hat{\vS}_i = \frac{1}{2} \sum_{\alpha \beta} \hat{c}^\dagger_{i \alpha} \hat{\vsi}_{\alpha \beta} \hat{c}_{i \beta},
\end{equation}
where $\hat{\vS}_i = \left( \hat{S}^x_i, \hat{S}^y_i, \hat{S}^z_i \right)$ is a vector of the spin operators at the site $i$, $\hat{\vsi} = \left( \hat{\sigma}^x_i, \hat{\sigma}^y_i, \hat{\sigma}^z_i \right)$ is a vector of the Pauli matrices, while $\hat{c}^\dagger_{i \alpha}$ and $\hat{c}_{i \beta}$ are the fermionic creation and annihilation operators respectively at the site $i$. The indices $\alpha$ and $\beta$ can take the values $\spinup$ and $\spindown$ for the fermions, and are also used as the indices of the Pauli matrices. The valid states of the original spin Hamiltonian pose the constraint $\ev{\hat{n}_i} = 1$ for all sites $i$, where $\hat{n}_i = \sum_\alpha \hat{c}^\dagger_{i \alpha} \hat{c}_{i \alpha}$ is the number operator. To satisfy this constraint, the Gutzwiller projector~\cite{gutzwiller1963effect}
\begin{equation}
\hat{P}_\text{G} = \prod_i \hat{n}_i (2 - \hat{n}_i)
\end{equation}
is applied to any variational state for the fermionic Hamiltonian, and this kind of variational states are commonly referred to as the Gutzwiller projected states.

A variety of variational ansatzes originally developed for fermionic systems can be applied to spin systems in this approach. To start with, the Slater determinant~\cite{slater1929theory} is a general description of the ground state of non-interacting fermionic Hamiltonians, which can be represented using the creation operators, without the normalization~\cite{toulouse2007optimization}:
\begin{equation}
\ket{\psi_\text{S}} = \prod_{i = 1}^{2 N} \left( \sum_{j = 1}^{2 N} W^\text{S}_{i j} \hat{c}^\dagger_j \right) \ket{0},
\end{equation}
where $\ket{0}$ is the fermionic vacuum state, and $\mW^\text{S}$ is a $2 N \times 2 N$ matrix of variational parameters. The indices $i$ and $j$ run over both the spin value and the site, i.e., $i = (1 \spinup), \ldots, (N \spinup), (1 \spindown), \ldots, (N, \spindown)$. In VMC, we need to sample the fermionic configuration $\vn = (n_{1 \spinup}, \ldots, n_{N \spinup}, n_{1 \spindown}, \ldots, n_{N \spindown})$, and evaluate the wave function values of the samples. Following the anti-commutation relations of fermions, the wave function value is given by a determinant:
\begin{align}
\ip{\vn}{\psi_\text{S}} &= \bra{0} \hat{c}_{x_N} \cdots \hat{c}_{x_1} \left( \sum_p (-1)^p \prod_{k = 1}^N W^\text{S}_{x_k, x_{p(k)}} \right) \hat{c}^\dagger_{x_1} \cdots \hat{c}^\dagger_{x_N} \ket{0} \\
&= \det \begin{pmatrix}
W^\text{S}_{x_1 x_1} & \cdots & W^\text{S}_{x_1 x_N} \\
\vdots & \ddots & \vdots \\
W^\text{S}_{x_N x_1} & \cdots & W^\text{S}_{x_N x_N}
\end{pmatrix},
\end{align}
where $\vx$ = $(x_1, \ldots, x_N)$ is a vector of indices where a fermion exists, and there must be $N$ fermions in the $2 N$ possible values of $i$, because of the constraint $\ev{\hat{n}_i} = 1$. The permutation $p$ runs over all possible permutations of $(1, \ldots, N)$. Before VMC optimization, the value of $\mW^\text{S}$ can be initialized as the unitary matrix that diagonalizes the quadratic part of the Hamiltonian, and it can still be a sensible initial value in some cases even if the Hamiltonian contains interactions between sites~\cite{yokoyama1987variational, gross1987Antiferromagnetic, ferrari2022charge}.

For Hamiltonians with electron pairing interactions as in the Bardeen--Cooper--Schrieffer (BCS) theory, a more precise description is the pair-product (PP) state, also known as the geminal wave function~\cite{bouchaud1988pair, gros1988superconductivity, tahara2008variational, astrakhantsev2021broken}:
\begin{equation}
\ket{\psi_\text{PP}} = \left( \sum_{i, j = 1}^{2 N} W^\text{PP}_{i j} \hat{c}^\dagger_i \hat{c}^\dagger_j \right)^{\frac{N}{2}} \ket{0},
\end{equation}
where $\mW^\text{PP}$ is a $2 N \times 2 N$ matrix of variational parameters. The wave function value is given by a Pfaffian~\cite{cayley1849determinants}, which is a property similar to the determinant but defined specifically for skew-symmetric matrices. Both the determinant and the Pfaffian can be evaluated in $O(N^3)$ time. When we only need the ratio of two probabilities, e.g., for the MCMC sampling in \cref{eq:metropolis}, it can be evaluated more efficiently without the whole determinant or Pfaffian~\cite{becca2017quantum5}.

Based on the above ansatzes, which are constructed from the analytical descriptions of the ground states of simple Hamiltonians, we can multiply more terms on them, commonly referred to as correlators, with more variational parameters to improve their expressiveness. A simple example is the Jastrow correlator:
\begin{equation}
\hat{C}_\text{J} = \exp\left( {\frac{1}{2} \sum_{i j} W^\text{J}_{i j} \hat{S}_i \hat{S}_j} \right),
\end{equation}
which is used in the well-known Slater--Jastrow ansatz:
\begin{equation}
\ket{\psi_\text{SJ}} = \hat{C}_\text{J} \ket{\psi_\text{S}}.
\end{equation}
The restricted Boltzmann machine (RBM) to be discussed in \cref{sec:nqs} can also be used as a correlator, which gives higher accuracy than the traditional Slater--Jastrow ansatz in modern VMC computations, especially for supposed spin liquids with the presence of disorder~\cite{nomura2017restricted, ferrari2019neural, nomura2021dirac}. We refer to Ref.~\cite{becca2017quantum5} as a self-contained introduction to VMC on fermionic systems, and to Ref.~\cite{ferrari2019static} for details of applying this approach on spin systems.

\subsection{Neural quantum states (NQSs)}
\label{sec:nqs}

The neural networks discussed in \cref{sec:nn} have also been proposed as variational ansatzes for quantum many-body systems, commonly known as neural quantum states (NQSs). A simple yet effective architecture is the restricted Boltzmann machine (RBM), which is essentially a two-layer neural network~\cite{carleo2017solving}:
\begin{align}
\ln \psi_\text{RBM}(\vs) &= \sum_i x_i + \sum_j a_j s_j, \\
x_i &= \ln 2 \cosh\left( b_i + \sum_j W_{i j} s_j \right).
\end{align}
Its variational parameters include $\mW, \va, \vb$, where $\mW$ is an $\alpha N \times N$ matrix, $\alpha$ is a hyperparameter controlling the size of the network, $\va$ is a vector known as the visible bias, and $\vb$ is another vector known as the hidden bias. When $\alpha = 1$, the RBM has approximately the same number of parameters as the Jastrow ansatz and achieves a higher accuracy than the latter on many physical systems~\cite{wu2024variational}. It has a deep physical background rooted in the statistical properties of disordered spin systems~\cite{sherrington1975solvable, hopfield1982neural}, and its simple architecture has motivated studies to interpret the learned parameters~\cite{abdollahi2016explainable, fernandez2023disentangling}. Despite its simplicity, it is still continuously used in recent VMC computations~\cite{nomura2017restricted, nomura2021dirac}.

A growing variety of NQS architectures have emerged in recent literature, including deeper multilayer architectures to improve the expressiveness~\cite{carleo2018constructing, sharir2022neural}, as well as convolutional neural networks (CNNs)~\cite{liang2018solving, choo2019two} and group equivariant neural networks~\cite{luo2021gauge, roth2021group} to implement symmetries of physical systems. In particular, Ref.~\cite{vieijra2021many} has proposed an architecture named ClebschTree to incorporate the non-abelian $\mathrm{SU}(2)$ symmetry into NQS. Meanwhile, autoregressive neural networks (ARNNs)~\cite{sharir2020deep} and recurrent neural networks (RNNs)~\cite{hibat2020recurrent, roth2020iterative} have been used to efficiently approximate complicated many-body distributions while supporting exact sampling, as discussed in \cref{ch:arnn}. The advancement in language modeling has also motivated the use of the transformer architecture in NQS~\cite{luo2021gauge, zhang2023transformer}.

A recent breakthrough has been made by the vision transformer (ViT)~\cite{viteritti2023transformer, viteritti2023transformer2, cao2024vision}, which greatly improves the results on the long-standing difficult problem of learning the sign structures in disordered systems, as we will discuss in \cref{sec:amp-phase}. Compared to the original ViT in vision-language modeling, the ViT used as NQS does not employ the autoregressive architecture, and many non-linear activation functions are removed~\cite{rende2024queries}. Its success can be attributed to the linear transformations arranged in a hierarchical scheme.

\section{Amplitude and phase of quantum ansatz}
\label{sec:amp-phase}

In the above discussion on quantum ansatzes, we did not emphasize that they are complex valued. In fact, we tend to avoid the intricacy of complex-valued functions aforementioned in \cref{sec:cmpl-grad}, especially when working with deep neural networks. A common practice is to parameterize the amplitude and the phase\footnote{In this section we refer to the phase of complex number, not the phase of matter.} of the wave function ansatz respectively using real-valued functions:
\begin{equation}
\psi(\vs) = \sqrt{q(\vs)}\,\rme^{\rmi \phi(\vs)},
\label{eq:psi-q-phi}
\end{equation}
where $q(\vs)$ can be interpreted as a probability distribution. It is more common to separate the amplitude and the phase, rather than the real and the imaginary parts, because we frequently work with the log-gradient in \cref{eq:vmc-grad-cmpl}, and the ratio of two probabilities in MCMC sampling as in \cref{eq:metropolis}. When sampling the configurations $\vs$, we only need to evaluate $q(\vs)$, and not $\phi(\vs)$. In particular, when parameterizing $q(\vs)$ using an AR model as in \cref{eq:autoreg}, there is no need for $\phi(\vs)$ to also fulfill the AR property.

This separation of the amplitude and the phase has been applied in a lot of literature on NQS~\cite{torlai2018neural, hibat2020recurrent, astrakhantsev2021broken}. In particular, Ref.~\cite{bukov2021learning} provides empirical evidence that an ansatz with this separation achieves higher accuracy than the corresponding ansatz with the complex parameterization, and Ref.~\cite{wang2024variational} even shows that the complex parameterization is outperformed by only optimizing the amplitude while fixing the phase according to the Marshall sign rule. On the other hand, there is also a lot of literature using the complex parameterization to achieve high accuracies on various systems~\cite{roth2020iterative, li2022bridging, viteritti2023transformer}, which typically use deeper networks and more parameters than the formers. Moreover, it remains an open question whether there is benefit in sharing a part of the parameters between the amplitude and the phase networks.

Although optimizing the amplitude can already be a difficult problem~\cite{bukov2021learning, park2022expressive}, optimizing the phase for non-stoquastic Hamiltonians is more intricate~\cite{westerhout2020generalization, szabo2020neural}. The ground state $\psi_0(\vs)$ must be real if the Hamiltonian is Hermitian, but its sign can have a complicated dependence on $\vs$, which poses challenge to the generalization ability of the neural network, i.e., predicting $\psi_0(\vs)$ of unseen configurations from the sampled configurations in training, even if the sampling is unbiased. This intricacy also comes from the fact that the signs have to be optimized either in the complex plane, or using discrete and combinatorial optimization methods beyond gradient descent (GD).

It has been practically shown in many cases that applying the Marshall sign rule is necessary for obtaining the correct sign structure~\cite{carleo2017solving, choo2019two, bukov2021learning, li2022bridging}, and it has been believed that it is more difficult for deeper neural networks to obtain the sign structure~\cite{szabo2020neural}. Only with the recent breakthrough of ViT, it has been achieved to learn the sign structures in disordered systems by GD without manually applying the Marshall sign rule~\cite{viteritti2023transformer, viteritti2023transformer2}.

To ease the optimization of the sign by GD, Ref.~\cite{szabo2020neural} proposes a phasor sum as the last layer of the phase network $\phi(\vs)$:
\begin{equation}
\phi(\vs) = \arg \sum_i \exp\left( b + \sum_i w_i \phi_i \right),
\label{eq:phasor-sum}
\end{equation}
where $\{\phi_i\}$ are the outputs of the previous layer, $\{w_i\}$ and $b$ are trainable parameters, and in numerical experiments it outperforms the usual sum
\begin{equation}
\phi(\vs) = b + \sum_i w_i \phi_i.
\end{equation}
It is also helpful to train the phase and the amplitude separately in a two-stage training procedure~\cite{szabo2020neural}, or alternatively train them for more stages in an ``alternating learning'' procedure~\cite{astrakhantsev2021broken}. To obtain the sign structure in the disordered regime, we can use the known sign structure from the neighboring regime as the initialization in an ``hysteresis optimization'' procedure~\cite{astrakhantsev2021broken}, or fine-tune only a part of the network after such initialization~\cite{rende2024fine}.

Beyond GD optimization in the usual setting of VMC, the signs can also be optimized in the setting of supervised learning, which uses a cross-entropy loss with known results~\cite{park2022expressive}, e.g., from exact diagonalization (ED) or highly accurate QMC on small systems, and then generalize to larger systems. It remains an open question whether this method can be used in unsupervised learning, possibly with modern estimators to relax the discrete signs into continuous and trainable variables~\cite{maddison2017concrete, jang2017categorical, tucker2017rebar, grathwohl2018backpropagation}. Alternatively, the signs can be obtained by combinatorial optimization~\cite{westerhout2023many} or evolutionary algorithm~\cite{chen2022neural}.

Although these methods have successful results on many physical systems, the understanding is still limited on the relation between the effectiveness of these methods and the properties of the physical system as well as the neural network. We also remark that even if we know the true sign structure, the sign problem aforementioned in \cref{sec:sign-problem} can still occur in the difficulty of reliably estimating the energy in \cref{eq:vmc} with a highly oscillating $E_\text{loc}(\vs)$.
